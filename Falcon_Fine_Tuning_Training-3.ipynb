{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Finetune Falcon on a Google colab\n",
        "\n",
        "Let's leverage PEFT library and QLoRA for more memory efficient finetuning."
      ],
      "metadata": {
        "id": "C2EgqEPDQ8v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models."
      ],
      "metadata": {
        "id": "i-tTvEF1RT3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "qodBMYkqdCpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNnkgBq7Q3EU"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "For our experiment, used description data from publically available details.\n"
      ],
      "metadata": {
        "id": "Rnqmq7amRrU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "mtdoWvD1G26K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('gdrive/My Drive/Dataset/code_snippets.csv')\n",
        "#df = pd.read_excel('gdrive/My Drive/Dataset/Doc_Dataset_final.xlsx')\n",
        "df = pd.read_excel('gdrive/My Drive/Dataset/')\n",
        "\n",
        "\n",
        "df_1 = df[['question', 'answer']]\n",
        "df_1.isna().sum()\n",
        "df_1 = df_1.dropna()\n",
        "\n",
        "data_1 = []\n",
        "\n",
        "for index, row in list(df_1.iterrows()):\n",
        "    data_1.append(dict(row))\n",
        "\n",
        "data = {\n",
        "  \"questions\": data_1,\n",
        "}\n",
        "\n",
        "with open(\"dataset.json\", 'w+') as f:\n",
        "    json.dump(data[\"questions\"], f)\n",
        "\n",
        "data = load_dataset('json', data_files='dataset.json')\n",
        "\n",
        "#print(data[\"train\"])\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "  return f\"\"\"\n",
        "<human>: {data_point[\"question\"]}\n",
        "<assistance>: {data_point[\"answer\"]}\n",
        "\"\"\".strip()\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "  full_prompt = generate_prompt(data_point)\n",
        "  value = print(full_prompt)\n",
        "  return value\n",
        "\n",
        "\n",
        "\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"dataset.csv\")\n",
        "\n",
        "dataset = load_dataset('csv', data_files='dataset.csv', split='train')"
      ],
      "metadata": {
        "id": "6GU_M1G-d6IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "rjOMoSbGSxx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it."
      ],
      "metadata": {
        "id": "AjB0WAqFSzlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "#model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "#model_name = \"tiiuae/falcon-7b\"\n",
        "model_name = \"tiiuae/falcon-40b-instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "ZwXZbQ2dSwzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's also load the tokenizer below"
      ],
      "metadata": {
        "id": "xNqIYtQcUBSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XDS2yYmlUAD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing before fine-tuning"
      ],
      "metadata": {
        "id": "SToT9dnchBBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda:0\""
      ],
      "metadata": {
        "id": "NdJvvoINbJZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "prompt = f\"\"\"\n",
        "<human>: How to create a molecule using rdkit?\n",
        "<assistance>:\n",
        "\"\"\".strip()\n",
        "\n",
        "# A program that performs a multi-threaded matched pair analysis of a set of structures for\n",
        "# Last updated on May 15, 2023.\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_token = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = generation_config.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.max_length = 200\n",
        "\n",
        "with torch.inference_mode():\n",
        "  outputs = model.generate(\n",
        "      input_ids = encoding.input_ids,\n",
        "      attention_mask = encoding.attention_mask,\n",
        "      generation_config = generation_config\n",
        "  )\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "bkjK3ilBa1kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer."
      ],
      "metadata": {
        "id": "NuAx3zBeUL1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the trainer"
      ],
      "metadata": {
        "id": "dzsYHLwIZoLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters."
      ],
      "metadata": {
        "id": "aTBJVE4PaJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "output_dir=\"gdrive/My Drive/falcon-adapter-output-new-1\"\n",
        "#output_dir = \"./instruct-falcon\"\n",
        "#output_dir = './output'\n",
        "per_device_train_batch_size = 8\n",
        "gradient_accumulation_steps = 4\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 20\n",
        "logging_steps = 5\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 100\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "gradient_checkpointing = True\n",
        "group_by_length = True\n",
        "save_total_limit = 40\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    #push_to_hub = True,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")"
      ],
      "metadata": {
        "id": "OCFTvGW6aspE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ],
      "metadata": {
        "id": "geyLDXMUJUG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "68Nsw8p4VzcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then finally pass everthing to the trainer"
      ],
      "metadata": {
        "id": "I3t6b2TkcJwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 512\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "TNeOBgZeTl2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ],
      "metadata": {
        "id": "GWplqqDjb3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)"
      ],
      "metadata": {
        "id": "7OyIvEx7b1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "1JApkSrCcL3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_kbS7nRxcMt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.push_to_hub('falcon-40B-instruct-600steps', create_pr=1)"
      ],
      "metadata": {
        "id": "7G1V3Td1a-P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model performance"
      ],
      "metadata": {
        "id": "nWIy-Sdki05Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DEVICE = \"cuda:0\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "Hi0fCsFrxh8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = f\"\"\"\n",
        "<human>: Which molecule type should I use in order to get the smallest memory usage for my database application?\n",
        "<assistance>:\n",
        "\"\"\".strip()\n",
        "\n",
        "#A program that performs a multi-threaded matched pair analysis of a set of structures for\n",
        "#Last updated on May 15, 2023.\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)"
      ],
      "metadata": {
        "id": "lPIBMPTExh_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = model.generation_config\n",
        "generation_config.max_new_token = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = generation_config.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.max_length = 200"
      ],
      "metadata": {
        "id": "tIE1A_hh3Gz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "  outputs = trainer.model.generate(\n",
        "      input_ids = encoding.input_ids,\n",
        "      attention_mask = encoding.attention_mask,\n",
        "      generation_config = generation_config\n",
        "  )\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "mWhTe49Z2SQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuCurgkOysMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-uIDXmL_cUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}