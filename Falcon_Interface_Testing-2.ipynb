{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP9S0m4q2J3R1vN8L7oP5pQ6R7T",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "e_z0qZ0aX-m-"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Falcon_Interface_Testing.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Heiim_2G7lSoKJnkfQMOYABRsnUE4rHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "## Finetune Falcon on a Google colab\n",
    "\n",
    "Let's leverage PEFT library and QLoRA for more memory efficient finetuning.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nvidia-smi"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-libs"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "notebook-login"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loading-model"
   },
   "source": [
    "## Loading the model\n",
    "\n",
    "In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load-model-tokenizer"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "#model_name = \"/falcon-7B-instruct-300steps-merged\"\n",
    "model_name = \"/falcon-7B-instruct-600steps-merged\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-tokenizer"
   },
   "source": [
    "# Let's also load the tokenizer below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tokenizer-init"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing"
   },
   "source": [
    "# Testing after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "inference",
    "colab": {
     "base_uri": "https://localhost/",
     "height": 200
    },
    "outputId": "9b9b9b9b-9c9c-9d9d-9e9e-9f9f9f9f9f9f"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%time\n",
    "prompt = f\"\"\"\n",
    "<human>: How to create a molecule\n",
    "<assistance>:\n",
    "\"\"\".strip()\n",
    "\n",
    "# A program that performs a multi-threaded matched pair analysis of a set of structures for\n",
    "# Last updated on May 15, 2023.\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_token = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.max_length = 200\n",
    "\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora-config"
   },
   "source": [
    "Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer."
   ]
  }
 ]
}
