{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNyN1UqX9tB2M7Y4C6fD8hI0J1K",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "e_z0qZ0aX-m-"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"LLM_Stopping_Criteria.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1MbsNP41fMrseoag5vEVn0LnfOJAwpay6\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-libs"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq transformers==4.30.0 --progress-bar off\n",
    "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
    "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
    "!pip install -qqq einops==0.6.1 --progress-bar off\n",
    "!pip install -qqq langchain==0.0.233 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseOutputParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "huggingface-login"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#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\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model-load"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_NAME = \"falcon-7B-instruct-300steps-merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tokenizer-load"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "generation-config"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.temperature = 0\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.repetition_penalty = 1.7\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "try-model"
   },
   "source": [
    "## Try the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model-test-prompt"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "The following is a friendly conversation between a human and an AI. The AI is\n",
    "talkative and provides lots of specific details from its context.\n",
    "\n",
    "Current conversation:\n",
    "\n",
    "Human: Who is Dwight K Schrute?\n",
    "AI:\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stop-rambling"
   },
   "source": [
    "## Stop the LLM From Rambling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "stop-criteria-class"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(\n",
    "        self, tokens: List[List[str]], tokenizer: AutoTokenizer, device: torch.device\n",
    "    ):\n",
    "        stop_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in tokens]\n",
    "        self.stop_token_ids = [\n",
    "            torch.tensor(x, dtype=torch.long, device=device) for x in stop_token_ids\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
    "    ) -> bool:\n",
    "        for stop_ids in self.stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "stop-tokens-setup"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_tokens = [[\"Human\", \":\"], [\"AI\", \":\"]]\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopGenerationCriteria(stop_tokens, tokenizer, model.device)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "generation-pipeline"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "llm-huggingface-pipeline"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "llm-test-prompt"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = llm(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conversation-chain"
   },
   "source": [
    "## Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chain-init"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain = ConversationChain(llm=llm)\n",
    "print(chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-prompt"
   },
   "source": [
    "## Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "custom-prompt-memory"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "The following is a conversation between a human an AI.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\".strip()\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"history\", k=6, return_only_outputs=True\n",
    ")\n",
    "\n",
    "chain = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chain-predict"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"how to create a molecule using rdkit\"\n",
    "res = chain.predict(input=text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleaning-output"
   },
   "source": [
    "## Cleaning Output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cleanup-output-parser"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CleanupOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        user_pattern = r\"\\nUser\"\n",
    "        text = re.sub(user_pattern, \"\", text)\n",
    "        human_pattern = r\"\\nHuman:\"\n",
    "        text = re.sub(human_pattern, \"\", text)\n",
    "        ai_pattern = r\"\\nAI:\"\n",
    "        return re.sub(ai_pattern, \"\", text).strip()\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"output_parser\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cleanup-chain"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"history\", k=6, return_only_outputs=True\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    output_parser=CleanupOutputParser(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cleanup-test"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "how to create a molecule using rdkit\n",
    "\"\"\".strip()\n",
    "res = chain(text)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "res-keys"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "print-response"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(res[\"response\"])"
   ]
  }
 ]
}